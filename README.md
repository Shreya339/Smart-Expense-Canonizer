# Smart Expense Canonizer â€” Trustâ€‘First AI Classification System

Smart Expense Canonizer is a **financialâ€‘grade, explainable AI system** that classifies expense transactions while prioritizing **trust, safety, and auditability** over raw automation.

This project is intentionally designed **not** as a â€œsmart model demoâ€, but as a **productionâ€‘style AI decision system** suitable for fintech environments.

---

## ðŸ”‘ Core Philosophy

> **Accuracy is not enough. Trust is the real problem.**

Every design decision in this system follows four principles:

- **Trust before flash**
- **Explainability over opacity**
- **Humanâ€‘inâ€‘theâ€‘loop by default**
- **Fail safe, never silently**

The system is built to **know when not to guess**.

---

## ðŸ§  What the System Does

When a user submits an expense description:

- The system classifies the transaction
- Computes **confidence** and **risk**
- Generates **humanâ€‘readable evidence**
- Flags uncertainty instead of guessing
- Logs decisions for auditability
- Learns safely from corrections

Instead of saying:

> â€œThis is Meals & Entertainmentâ€

It says:

> â€œThis is Meals & Entertainment, with 92% confidence,  
> because similar merchants were classified this way in the past.â€

---

## ðŸ—ï¸ Highâ€‘Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit â”‚   â† UI
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI   â”‚   â† API Layer
â”‚  /classify â”‚
â”‚ /counterf. â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Classification Pipeline        â”‚
â”‚                                       â”‚
â”‚ 1. PII Redaction                      â”‚
â”‚ 2. Text Normalization                 â”‚
â”‚ 3. Embedding Similarity (Memory)      â”‚
â”‚ 4. Deterministic Rules Engine         â”‚
â”‚ 5. LLM Fallback (OpenAI â†’ Gemini)     â”‚
â”‚ 6. Confidence & Risk Scoring          â”‚
â”‚ 7. Evidence Generation                â”‚
â”‚ 8. Drift Awareness                    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQLite   â”‚  â† Audit Log / Memory
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§© Decision Layers (In Order)

### 1ï¸âƒ£ PII Redaction
Sensitive data (emails, phone numbers, etc.) is removed **before any AI decision**.

### 2ï¸âƒ£ Embedding Similarity (Primary Path)
- Merchant descriptions are embedded
- Compared against historical transactions
- High similarity â‡’ **reuse past category confidently**
- Fast, cheap, consistent, explainable

### 3ï¸âƒ£ Rules Engine (Deterministic Safety Net)
- Explicit mappings (e.g., Uber â†’ Travel)
- Predictable and auditable behavior
- Prevents unnecessary LLM calls

### 4ï¸âƒ£ LLM Fallback (Last Resort)
- OpenAI called first (selfâ€‘consistency logic)
- Gemini used as fallback
- Strict JSON validation + category whitelist
- If models fail â†’ **Needs Review**

AI is used **only when necessary**.

---

## ðŸ›¡ï¸ Trust & Safety Outputs

Every classification returns:

- **Confidence Score** (model certainty)
- **Risk Score** (systemâ€‘level safety)
- **Needs Review flag**
- **Evidence Trail** (why this decision happened)
- **Source Attribution** (embedding / rules / LLM)

No silent failures. No black boxes.

---

## ðŸ” Risk Scoring Logic

Risk increases when:
- Confidence is low
- Similarity is weak
- Merchant is unseen
- Overrides are frequent
- Drift is detected
- Validation flags appear

Risk is capped at **1.0** and mapped to:
- Low
- Medium
- High

---

## ðŸ” Counterfactual Reasoning

The `/counterfactual` endpoint answers:

> â€œIf the description changed slightly, would the category change?â€

This detects **fragile or unstable decisions**, which is critical in financial workflows.

---

## ðŸ§ª Testing Strategy

- **Unit tests** for:
  - Risk scoring
  - Evidence generation
  - Guardrails
- **Integration tests** for:
  - `/classify` endpoint
  - Endâ€‘toâ€‘end API behavior

Tests are written with clarity and comments â€” like production code.

---

## ðŸš€ Running the Project

Provide OPENAI_API_KEY and GEMINI_API_KEY in backend/.env

### VENV
```
.venv\Scripts\activate
```

### Backend
```bash
python -m uvicorn backend.main:app --reload
```

### UI
```bash
cd ui
streamlit run app.py
```

### Tests
```bash
pytest
```
### DB
```bash
sqlite3 transactions.db
.tables
DELETE FROM merchantembedding;
DELETE FROM "transaction";
```
---

## ðŸ“Œ Why This Matters

Financial systems cannot afford:
- Confidently wrong AI
- Silent failures
- Unexplainable decisions

This system is built to:
- Surface uncertainty
- Preserve human control
- Earn trust incrementally

---

## âœ¨ Final Note

This project reflects how I think about **responsible AI systems**:

> Not just *what* the model predicts â€”  
> but *when it should refuse*, *how it explains itself*,  
> and *how humans stay in control*.

That mindset is what turns AI from a demo into a platform.
